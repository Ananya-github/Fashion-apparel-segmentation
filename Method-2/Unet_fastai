{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing Modules\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\nfrom PIL import Image,ImageFont\n%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\n\nimport os\nimport sys\nimport random\nimport math\nimport numpy as np\nimport skimage.io\nfrom skimage.color import rgb2gray\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport cv2\nfrom scipy import ndimage\n\n# Root directory of the project\nROOT_DIR = os.path.abspath(\"../\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-16T16:48:50.647787Z","iopub.execute_input":"2021-08-16T16:48:50.648413Z","iopub.status.idle":"2021-08-16T16:48:52.284733Z","shell.execute_reply.started":"2021-08-16T16:48:50.648358Z","shell.execute_reply":"2021-08-16T16:48:52.283803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_path = '/kaggle/input/imaterialist-fashion-2019-FGVC6/'\n#reading train csv file\ntrain_df = pd.read_csv(os.path.join(root_path, 'train.csv'))\ntrain_df.shape","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-16T16:48:52.286553Z","iopub.execute_input":"2021-08-16T16:48:52.286986Z","iopub.status.idle":"2021-08-16T16:49:19.289159Z","shell.execute_reply.started":"2021-08-16T16:48:52.286954Z","shell.execute_reply":"2021-08-16T16:49:19.288391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:19.290169Z","iopub.execute_input":"2021-08-16T16:49:19.290567Z","iopub.status.idle":"2021-08-16T16:49:19.310593Z","shell.execute_reply.started":"2021-08-16T16:49:19.290528Z","shell.execute_reply":"2021-08-16T16:49:19.309299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:19.311822Z","iopub.execute_input":"2021-08-16T16:49:19.312101Z","iopub.status.idle":"2021-08-16T16:49:19.453467Z","shell.execute_reply.started":"2021-08-16T16:49:19.312073Z","shell.execute_reply":"2021-08-16T16:49:19.452655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"no null values","metadata":{}},{"cell_type":"code","source":"num_test_images = len(os.listdir(os.path.join(root_path,'test')))\nnum_train_images = len(os.listdir(os.path.join(root_path,'train')))\nprint(\"Number of images in test set: {}\".format(num_test_images))\nprint(\"Number of images in train set: {}\".format(num_train_images))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:19.455898Z","iopub.execute_input":"2021-08-16T16:49:19.456333Z","iopub.status.idle":"2021-08-16T16:49:21.020426Z","shell.execute_reply.started":"2021-08-16T16:49:19.456294Z","shell.execute_reply":"2021-08-16T16:49:21.019254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_class_per_image = np.round(train_df.shape[0]/num_train_images, 2)\nprint(\"Average number of classes per image: {}\".format(avg_class_per_image))\nassert len(train_df[\"ImageId\"].value_counts()) == num_train_images\nprint(\"Every image has at least 1 class\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.022951Z","iopub.execute_input":"2021-08-16T16:49:21.023303Z","iopub.status.idle":"2021-08-16T16:49:21.144636Z","shell.execute_reply.started":"2021-08-16T16:49:21.023268Z","shell.execute_reply":"2021-08-16T16:49:21.143872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading categories\nwith open(os.path.join(root_path, 'label_descriptions.json')) as f:\n    labels_data=json.load(f)\nlabels_data","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.145702Z","iopub.execute_input":"2021-08-16T16:49:21.146086Z","iopub.status.idle":"2021-08-16T16:49:21.18326Z","shell.execute_reply.started":"2021-08-16T16:49:21.146055Z","shell.execute_reply":"2021-08-16T16:49:21.182396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#separating the categories and attributes\ncategories = pd.DataFrame(labels_data['categories'])\nattributes = pd.DataFrame(labels_data['attributes'])\nprint(\"There are descriptions for\", categories.shape[0],\"categories and\", attributes.shape[0], \"attributes\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.184727Z","iopub.execute_input":"2021-08-16T16:49:21.185048Z","iopub.status.idle":"2021-08-16T16:49:21.197943Z","shell.execute_reply.started":"2021-08-16T16:49:21.185017Z","shell.execute_reply":"2021-08-16T16:49:21.196835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.199466Z","iopub.execute_input":"2021-08-16T16:49:21.199927Z","iopub.status.idle":"2021-08-16T16:49:21.214021Z","shell.execute_reply.started":"2021-08-16T16:49:21.199882Z","shell.execute_reply":"2021-08-16T16:49:21.212753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attributes.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.215737Z","iopub.execute_input":"2021-08-16T16:49:21.216055Z","iopub.status.idle":"2021-08-16T16:49:21.234335Z","shell.execute_reply.started":"2021-08-16T16:49:21.216026Z","shell.execute_reply":"2021-08-16T16:49:21.233072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories['supercategory'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.23582Z","iopub.execute_input":"2021-08-16T16:49:21.236256Z","iopub.status.idle":"2021-08-16T16:49:21.246398Z","shell.execute_reply.started":"2021-08-16T16:49:21.236202Z","shell.execute_reply":"2021-08-16T16:49:21.245448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attributes['supercategory'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.247732Z","iopub.execute_input":"2021-08-16T16:49:21.24817Z","iopub.status.idle":"2021-08-16T16:49:21.261912Z","shell.execute_reply.started":"2021-08-16T16:49:21.248124Z","shell.execute_reply":"2021-08-16T16:49:21.260956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#separating categories and attributes in train data\ntrain_df['hasAttributes'] = train_df.ClassId.apply(lambda x: x.find(\"_\") > 0)\ntrain_df['CategoryId'] = train_df.ClassId.apply(lambda x: x.split(\"_\")[0]).astype(int)\ntrain_df = train_df.merge(categories, left_on=\"CategoryId\", right_on=\"id\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.263019Z","iopub.execute_input":"2021-08-16T16:49:21.263377Z","iopub.status.idle":"2021-08-16T16:49:21.791955Z","shell.execute_reply.started":"2021-08-16T16:49:21.263346Z","shell.execute_reply":"2021-08-16T16:49:21.790915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fine_grained_obj_perc = np.round(train_df[\"hasAttributes\"].mean()*100, 1)\nprint(\"{}% of the objects are fine-grained.\".format(fine_grained_obj_perc))\nfine_grained_img_perc = np.round((train_df.groupby(\"ImageId\")[\"hasAttributes\"].sum() > 0).mean()*100, 1)\nprint(\"{}% of the images have at least one fine-grained object.\".format(fine_grained_img_perc))\nclass_df = train_df.groupby(\"CategoryId\").agg({\"ImageId\": \"count\"}).reset_index()\nclass_df = class_df.rename(columns={\"ImageId\": \"img_count\"})\nprint(\"Number of classes: {}\".format(class_df.shape[0]))\nprint(\"{} of the classes are fine-grained.\".format(train_df[train_df[\"hasAttributes\"] == True].CategoryId.nunique()))\nclass_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:21.793116Z","iopub.execute_input":"2021-08-16T16:49:21.793402Z","iopub.status.idle":"2021-08-16T16:49:22.111585Z","shell.execute_reply.started":"2021-08-16T16:49:21.793375Z","shell.execute_reply":"2021-08-16T16:49:22.110471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eda\ndef plot_function_for_supercategories(subset,title):\n    supercategory_names = np.unique(subset.supercategory)\n    plt.figure(figsize=(10, 6))\n    g = sns.countplot(x = 'supercategory', data=subset, order=supercategory_names)\n    ax = g.axes\n    tl = [x.get_text() for x in ax.get_xticklabels()]    \n    ax.set_xticklabels(tl, rotation=45)\n    for p, label in zip(ax.patches, supercategory_names):\n        c = subset[(subset['supercategory'] == label)].shape[0]\n        ax.annotate(str(c), (p.get_x()+0.3, p.get_height() + 50))\n    plt.title(title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:22.112827Z","iopub.execute_input":"2021-08-16T16:49:22.11311Z","iopub.status.idle":"2021-08-16T16:49:22.12366Z","shell.execute_reply.started":"2021-08-16T16:49:22.113082Z","shell.execute_reply":"2021-08-16T16:49:22.122195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_function_for_supercategories(train_df[train_df.hasAttributes],'Supercategories with any attributes')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:22.125496Z","iopub.execute_input":"2021-08-16T16:49:22.125989Z","iopub.status.idle":"2021-08-16T16:49:22.332909Z","shell.execute_reply.started":"2021-08-16T16:49:22.12594Z","shell.execute_reply":"2021-08-16T16:49:22.331644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_function_for_supercategories(train_df[~train_df.hasAttributes],'Supercategories with no attributes')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:22.334638Z","iopub.execute_input":"2021-08-16T16:49:22.335112Z","iopub.status.idle":"2021-08-16T16:49:23.711086Z","shell.execute_reply.started":"2021-08-16T16:49:22.335064Z","shell.execute_reply":"2021-08-16T16:49:23.710097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"super_cat = list(train_df['supercategory'].unique())\nfig, axes = plt.subplots(6, 2, figsize=(25, 20))\nz=0\nfor i in range(0, 6):\n    for j in range(0, 2):\n        sns.countplot(y=\"name\", data=train_df[train_df.supercategory.isin([super_cat[z]])],ax = axes[i, j]).set(title = (super_cat[z]))\n        fig.tight_layout()\n        z=z+1","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:23.712548Z","iopub.execute_input":"2021-08-16T16:49:23.712894Z","iopub.status.idle":"2021-08-16T16:49:28.426569Z","shell.execute_reply.started":"2021-08-16T16:49:23.712861Z","shell.execute_reply":"2021-08-16T16:49:28.425638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## understanding and reading few images from train data","metadata":{}},{"cell_type":"code","source":"# reading sample images from training data\nfor i in range(6):\n    id_image=train_df['ImageId'].iloc[np.random.randint(0,train_df.shape[0])]\n    print('Image ID:',id_image)\n    image = plt.imread(os.path.join(root_path,'train/',id_image))\n    plt.imshow(image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:28.428208Z","iopub.execute_input":"2021-08-16T16:49:28.428537Z","iopub.status.idle":"2021-08-16T16:49:35.880496Z","shell.execute_reply.started":"2021-08-16T16:49:28.428505Z","shell.execute_reply":"2021-08-16T16:49:35.879484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = plt.imread(os.path.join(root_path,'train/','b98f08f330c23af5db1c62c2412592b4.jpg'))\ngray = rgb2gray(image)\nplt.imshow(gray, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:35.881793Z","iopub.execute_input":"2021-08-16T16:49:35.88207Z","iopub.status.idle":"2021-08-16T16:49:36.617218Z","shell.execute_reply.started":"2021-08-16T16:49:35.882043Z","shell.execute_reply":"2021-08-16T16:49:36.616247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gray_r = gray.reshape(gray.shape[0]*gray.shape[1])\nan_array = np.where(gray_r > gray_r.mean(), 0, 3)\ngray = an_array.reshape(gray.shape[0],gray.shape[1])\nplt.imshow(gray, cmap='binary_r')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:36.618371Z","iopub.execute_input":"2021-08-16T16:49:36.618642Z","iopub.status.idle":"2021-08-16T16:49:37.195602Z","shell.execute_reply.started":"2021-08-16T16:49:36.618615Z","shell.execute_reply":"2021-08-16T16:49:37.19476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# execution_path = '../input/imageai/resnet50_coco_best_v2.0.1.h5'\n# detector = ObjectDetection()\n# detector.setModelTypeAsRetinaNet()\n# detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\n# detector.loadModel()\n# detections = detector.detectObjectsFromImage(input_image=os.path.join(root_path,'train/','b98f08f330c23af5db1c62c2412592b4.jpg'), output_image_path=os.path.join(root_path,'b98f08f330c23af5db1c62c2412592b4_detection.jpg'))\n# pic = plt.imread(os.path.join(root_path,'b98f08f330c23af5db1c62c2412592b4_detection.jpg'))\n# plt.imshow(pic)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:37.196831Z","iopub.execute_input":"2021-08-16T16:49:37.197108Z","iopub.status.idle":"2021-08-16T16:49:37.201168Z","shell.execute_reply.started":"2021-08-16T16:49:37.19708Z","shell.execute_reply":"2021-08-16T16:49:37.200199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git clone https://www.github.com/matterport/Mask_RCNN.git\n# os.chdir('Mask_RCNN')\n# !rm -rf .git # to prevent an error when the kernel is committed\n# !rm -rf images assets # to prevent displaying images at the bottom of a kernel\n\n# # Root directory of the project\n# ROOT_DIR = os.path.abspath(\"../\")\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # Import Mask RCNN\n# sys.path.append(ROOT_DIR)  # To find local version of the library\n# from mrcnn import utils\n# import mrcnn.model as modellib\n# from mrcnn import visualize\n# # Import COCO config\n# sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n# import coco\n\n# # Directory to save logs and trained model\n# MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n\n# # Local path to trained weights file\n# COCO_MODEL_PATH = os.path.join('', \"mask_rcnn_coco.h5\")\n\n# # Download COCO trained weights from Releases if needed\n# if not os.path.exists(COCO_MODEL_PATH):\n#     utils.download_trained_weights(COCO_MODEL_PATH)\n\n# # Directory of images to run detection on\n# IMAGE_DIR = os.path.join(ROOT_DIR, \"trump.jpg\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:49:37.202736Z","iopub.execute_input":"2021-08-16T16:49:37.203085Z","iopub.status.idle":"2021-08-16T16:49:37.213828Z","shell.execute_reply.started":"2021-08-16T16:49:37.203053Z","shell.execute_reply":"2021-08-16T16:49:37.212888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install progressbar","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from progressbar import ProgressBar","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:52:13.288285Z","iopub.execute_input":"2021-08-16T16:52:13.288668Z","iopub.status.idle":"2021-08-16T16:52:13.309644Z","shell.execute_reply.started":"2021-08-16T16:52:13.288634Z","shell.execute_reply":"2021-08-16T16:52:13.307412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import required packages\nfrom pathlib import Path\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.utils.mem import *\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:28:48.208328Z","iopub.execute_input":"2021-08-16T16:28:48.208923Z","iopub.status.idle":"2021-08-16T16:31:22.202868Z","shell.execute_reply.started":"2021-08-16T16:28:48.208862Z","shell.execute_reply":"2021-08-16T16:31:22.199917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a folder for the mask images\nif  not os.path.isdir('../labels'):\n    os.makedirs('../labels')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:38.707563Z","iopub.execute_input":"2021-08-16T16:31:38.707969Z","iopub.status.idle":"2021-08-16T16:31:38.713875Z","shell.execute_reply.started":"2021-08-16T16:31:38.707937Z","shell.execute_reply":"2021-08-16T16:31:38.712499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:39.225746Z","iopub.execute_input":"2021-08-16T16:31:39.22611Z","iopub.status.idle":"2021-08-16T16:31:39.234027Z","shell.execute_reply.started":"2021-08-16T16:31:39.226076Z","shell.execute_reply":"2021-08-16T16:31:39.232665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = Path(\"../input/imaterialist-fashion-2019-FGVC6\")\n# path_img = path+'/train'\n# path_lbl = root_path+Path(\"../labels\")\n# only the 27 apparel items, plus 1 for background\n# model image size 224x224\ncategory_num = 27 + 1\nsize = 224\n# get and show categories\n# with open(os.path.join(root_path,\"label_descriptions.json\")) as f:\n#     label_descriptions = json.load(f)\n# label_names = [x['name'] for x in label_descriptions['categories']]\n# print(label_names)\n# train dataframe\ndf = train_df[['ImageId', 'EncodedPixels', 'Height', 'Width','ClassId']]","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:39.641484Z","iopub.execute_input":"2021-08-16T16:31:39.642004Z","iopub.status.idle":"2021-08-16T16:31:39.88051Z","shell.execute_reply.started":"2021-08-16T16:31:39.641972Z","shell.execute_reply":"2021-08-16T16:31:39.879407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training image path and images\nfnames = get_image_files(os.path.join(root_path,'train'))\nprint(fnames[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:40.088704Z","iopub.execute_input":"2021-08-16T16:31:40.089029Z","iopub.status.idle":"2021-08-16T16:33:45.258679Z","shell.execute_reply.started":"2021-08-16T16:31:40.088997Z","shell.execute_reply":"2021-08-16T16:33:45.257432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# need a function to turn the run encoded pixels from train.csv into an image mask\n# there are multiple rows per image for different apparel items, this groups them into one mask\ndef make_mask_img(segment_df):\n    seg_width = segment_df.at[0, \"Width\"]\n    seg_height = segment_df.at[0, \"Height\"]\n    seg_img = np.full(seg_width*seg_height, category_num-1, dtype=np.int32)\n    for encoded_pixels, class_id in zip(segment_df[\"EncodedPixels\"].values, segment_df[\"ClassId\"].values):\n        pixel_list = list(map(int, encoded_pixels.split(\" \")))\n        for i in range(0, len(pixel_list), 2):\n            start_index = pixel_list[i] - 1\n            index_len = pixel_list[i+1] - 1\n            if int(class_id.split(\"_\")[0]) < category_num - 1:\n                seg_img[start_index:start_index+index_len] = int(class_id.split(\"_\")[0])\n    seg_img = seg_img.reshape((seg_height, seg_width), order='F')\n    return seg_img","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:33:45.261212Z","iopub.execute_input":"2021-08-16T16:33:45.261794Z","iopub.status.idle":"2021-08-16T16:33:45.27523Z","shell.execute_reply.started":"2021-08-16T16:33:45.261737Z","shell.execute_reply":"2021-08-16T16:33:45.272323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can look at an image to see how the processing works\n# the original image\nimg_file = fnames[500]\nimg = open_image(img_file)\nimg.show(figsize=(5,5))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:33:45.277552Z","iopub.execute_input":"2021-08-16T16:33:45.278171Z","iopub.status.idle":"2021-08-16T16:33:45.506718Z","shell.execute_reply.started":"2021-08-16T16:33:45.278122Z","shell.execute_reply":"2021-08-16T16:33:45.505518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert rows for this image into a numpy array mask\nimg_name = os.path.basename(img_file)\nimg_df = df[df.ImageId == img_name].reset_index()\n#img_df = img_df.iloc[0:1]\n#img_df = img_df[img_df.ClassId.astype(int) < category_num - 1].reset_index()\nimg_mask = make_mask_img(img_df)\nplt.imshow(img_mask)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:33:45.508431Z","iopub.execute_input":"2021-08-16T16:33:45.509117Z","iopub.status.idle":"2021-08-16T16:33:45.881814Z","shell.execute_reply.started":"2021-08-16T16:33:45.509068Z","shell.execute_reply":"2021-08-16T16:33:45.880766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the numpy array into a three channel png that can be used in the standard SegmentationItemList\n# then write into the labels folder as png and show the image\n# all pixels have the category numbers, so it looks like a dark greyscale image\nimg_mask_3_chn = np.dstack((img_mask, img_mask, img_mask))\ncv2.imwrite('../labels/' + os.path.splitext(img_name)[0] + '_P.png', img_mask_3_chn)\npng = open_image('../labels/' + os.path.splitext(img_name)[0] + '_P.png')\npng.show(figsize=(5,5))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:33:45.885153Z","iopub.execute_input":"2021-08-16T16:33:45.885898Z","iopub.status.idle":"2021-08-16T16:33:46.06771Z","shell.execute_reply.started":"2021-08-16T16:33:45.885849Z","shell.execute_reply":"2021-08-16T16:33:46.066227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use fastai's open_mask for an easier-to-view image (and check it works...)\nmask = open_mask('../labels/' + os.path.splitext(img_name)[0] + '_P.png')\nmask.show(figsize=(5,5), alpha=1)\nprint(mask.data)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:33:46.069972Z","iopub.execute_input":"2021-08-16T16:33:46.070426Z","iopub.status.idle":"2021-08-16T16:33:46.16517Z","shell.execute_reply.started":"2021-08-16T16:33:46.070379Z","shell.execute_reply":"2021-08-16T16:33:46.163739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run the same procedure for a sample of first 5000 images in dataset\nimages = df.ImageId.unique()[:50]","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:33:55.00289Z","iopub.execute_input":"2021-08-16T16:33:55.003266Z","iopub.status.idle":"2021-08-16T16:33:55.055802Z","shell.execute_reply.started":"2021-08-16T16:33:55.003232Z","shell.execute_reply":"2021-08-16T16:33:55.054809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pbar = ProgressBar()\nfor img in pbar(images):\n    img_df = df[df.ImageId == img].reset_index()\n    img_mask = make_mask_img(img_df)\n    img_mask_3_chn = np.dstack((img_mask, img_mask, img_mask))\n    cv2.imwrite('../labels/' + os.path.splitext(img)[0] + '_P.png', img_mask_3_chn)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:34:02.821872Z","iopub.execute_input":"2021-08-16T16:34:02.822321Z","iopub.status.idle":"2021-08-16T16:34:02.843977Z","shell.execute_reply.started":"2021-08-16T16:34:02.82228Z","shell.execute_reply":"2021-08-16T16:34:02.841824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before creating the databunch we need a function to find the mask images\n# also set the batch size, categories and wd\nget_y_fn = lambda x: Path(\"../labels\")/f'{Path(x).stem}_P.png'\nbs = 32\n#classes = label_names\ncodes = list(range(category_num))\nwd = 1e-2","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:22.233132Z","iopub.status.idle":"2021-08-16T16:31:22.234115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the databunch\nimages_df = pd.DataFrame(images)\nsrc = (SegmentationItemList.from_df(images_df, os.path.join(root_path,'train'))\n       .split_by_rand_pct()\n       .label_from_func(get_y_fn, classes=codes))\n\ndata = (src.transform(get_transforms(), size=size, tfm_y=True)\n       .databunch(bs=bs)\n       .normalize(imagenet_stats))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:22.235913Z","iopub.status.idle":"2021-08-16T16:31:22.236884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at a batch\ndata.show_batch(3, figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:22.238731Z","iopub.status.idle":"2021-08-16T16:31:22.239695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I create an accuracy metric which excludes the background pixels\ndef acc_fashion(input, target):\n    target = target.squeeze(1)\n    mask = target != category_num - 1\n    return (input.argmax(dim=1)==target).float().mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:22.241279Z","iopub.status.idle":"2021-08-16T16:31:22.242134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learner, include where to save pre-trained weights (default is in non-write directory)\nlearn = unet_learner(data, models.resnet34, metrics=acc_fashion, wd=wd, model_dir=\"/kaggle/working/models\")","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:22.243684Z","iopub.status.idle":"2021-08-16T16:31:22.244784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run learning rate finder\nlr_find(learn)\nlearn.recorder.plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:22.246394Z","iopub.status.idle":"2021-08-16T16:31:22.247582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set learning rate based on roughly the steepest part of the curve\nlr=1e-3","metadata":{"execution":{"iopub.status.busy":"2021-08-16T16:31:22.249189Z","iopub.status.idle":"2021-08-16T16:31:22.250015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train for 10 cycles frozen\n# Change back to 10 instead of 1\nlearn.fit_one_cycle(1, slice(lr), pct_start=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T10:11:06.096314Z","iopub.execute_input":"2021-08-16T10:11:06.096697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at some results\nlearn.show_results()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unfreeze earlier weights\nlearn.unfreeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# decrease the learning rate\nlrs = slice(lr/400,lr/4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train for 10 more cycles unfrozen\nlearn.fit_one_cycle(10, lrs, pct_start=0.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# more results\nlearn.show_results()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/imaterialist-fashion-2019-FGVC6/test/0046f98599f05fd7233973e430d6d04d.jpg'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(learn.data.classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/imaterialist-fashion-2019-FGVC6/test/0146a53e12d690914995248fb6872121.jpg'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(x[2][27])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/personal-testing/1.jpeg'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/personal-group-1/_G2A0656.JPG'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/personal-2/MicrosoftTeams-image (6).png'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/personal-2/MicrosoftTeams-image (7).png'\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(x[2][27])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnames = get_image_files('../input/personal-girl-power')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = gnames[0]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = gnames[1]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = gnames[2]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = gnames[3]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = gnames[4]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = gnames[5]\nimg = open_image(test_path)\nx = learn.predict(img)\nimg.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(9, 3, figsize=(25, 20))\nz=0\nfor i in range(9):\n    for j in range(3):\n        axes[i,j].imshow(x[2][z])\n#         plt.imshow(x[2][z])\n        z=z+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
